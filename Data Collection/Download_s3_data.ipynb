{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4871,"status":"ok","timestamp":1696263086996,"user":{"displayName":"Test Qzense","userId":"10561881519641656595"},"user_tz":-330},"id":"BKGdgNQb_KmV","outputId":"c6fc5214-79f0-4bfa-e324-4bb0239a7db2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12644,"status":"ok","timestamp":1696262724202,"user":{"displayName":"Test Qzense","userId":"10561881519641656595"},"user_tz":-330},"id":"ISyrSQFa99e-","outputId":"0e20d068-1084-403b-b2fe-29c7be65d331"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting boto3\n","  Downloading boto3-1.28.57-py3-none-any.whl (135 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting botocore<1.32.0,>=1.31.57 (from boto3)\n","  Downloading botocore-1.31.57-py3-none-any.whl (11.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3)\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.8.0,>=0.7.0 (from boto3)\n","  Downloading s3transfer-0.7.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.32.0,>=1.31.57->boto3) (2.8.2)\n","Collecting urllib3<1.27,>=1.25.4 (from botocore<1.32.0,>=1.31.57->boto3)\n","  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.57->boto3) (1.16.0)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 2.0.4\n","    Uninstalling urllib3-2.0.4:\n","      Successfully uninstalled urllib3-2.0.4\n","Successfully installed boto3-1.28.57 botocore-1.31.57 jmespath-1.0.1 s3transfer-0.7.0 urllib3-1.26.16\n"]}],"source":["! pip install boto3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b_5p7KOL9JQe"},"outputs":[],"source":["import os\n","import boto3\n","from tqdm import tqdm\n","from datetime import datetime, timedelta\n","\n","# Set the environment variables\n","# use your AWS credentials insted of these\n","os.environ['AWS_ACCESS_KEY_ID'] = ''\n","os.environ['AWS_SECRET_ACCESS_KEY'] = ''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDd71Ply9Wp8"},"outputs":[],"source":["class S3DataDownloader:\n","    def __init__(self, bucket_name, download_path):\n","        \"\"\"\n","        Initializes the S3DataDownloader instance.\n","\n","        Args:\n","            bucket_name (str): The name of the S3 bucket.\n","            download_path (str): The local directory to download files to.\n","        \"\"\"\n","        self.bucket_name = bucket_name\n","        self.download_path = download_path\n","        # Create an S3 client\n","        self.s3_client = boto3.client('s3')\n","        self.grouped_objects = self.group_objects_by_date()\n","\n","    def list_s3_objects(self, bucket_name=None):\n","        \"\"\"\n","        Lists all objects in the specified S3 bucket.\n","\n","        Args:\n","            bucket_name (str): The name of the S3 bucket.\n","\n","        Returns:\n","            list: A list of objects in the S3 bucket.\n","        \"\"\"\n","        if bucket_name is None:\n","            bucket_name = self.bucket_name\n","        object_list = []\n","        # Use a paginator to iterate through all the objects in the bucket\n","        paginator = self.s3_client.get_paginator('list_objects_v2')\n","        page_iterator = paginator.paginate(Bucket=bucket_name)\n","\n","        for page in page_iterator:\n","            # Get the list of objects in the current page\n","            objects = page.get('Contents', [])\n","            object_list.extend(objects)\n","        print(\"\\nlist_s3_objects done\")\n","        return object_list\n","\n","    def group_objects_by_date(self):\n","        \"\"\"\n","        Lists all objects in the S3 bucket & groups them by their last modified date.\n","\n","        Returns:\n","            dict: A dictionary where keys are dates and values are lists of object keys.\n","        \"\"\"\n","        objects = self.list_s3_objects(bucket_name=self.bucket_name)\n","        grouped_objects = {}\n","        for obj in objects:\n","            key = obj['Key']\n","            timestamp = obj['LastModified']\n","            date = str(timestamp.date())\n","\n","            if date in grouped_objects:\n","                grouped_objects[date].append(key)\n","            else:\n","                grouped_objects[date] = [key]\n","\n","        # Sort the dictionary based on date values in the keys\n","        grouped_objects = dict(sorted(grouped_objects.items(),\n","                                      key=lambda item: item[0]))\n","        print(\"group_objects_by_date done\\n\")\n","        return grouped_objects\n","\n","    def replace_misspelled_folder_names(self, species_name):\n","        misspelled_folders = {\n","            'Basa' : ['Basa', 'Basaa'],\n","            'Are' : ['Ar', 'Are'],\n","            'Barracuda' : ['Barcoda', 'Barkoda', 'Barracoda', 'Barracuda'],\n","            'Bolo' : ['Bolo', 'Bulo'],\n","            'Sea bass' : ['C boss', 'C boos', 'Siba'],\n","            'Chara pona' : ['Chara pana'],\n","            'Emperor' : ['Comprel', 'Emperor', 'Emporwel', 'Empowel',\n","                         'M perl', 'M preal'],\n","            'Demo' : ['Demo', 'Demo2', 'Test', 'Trial'],\n","            'Hilsa' : ['Hilsa', 'Hilis'],\n","            'Catla' : ['Katala', 'Katalaa', 'Katla'],\n","            'Croaker' : ['Kokor', 'Croaker'],\n","            'Lady' : ['Lady', 'Ledi'],\n","            'Malabar trevally' : ['Mabar tavili', 'Malbhot',\n","                                  'Trvili', 'Trevally'],\n","            'Needle' : ['Needale', 'Nidal', 'Nidil'],\n","            'Parsi' : ['Parci'],\n","            'Pearl spot' : ['(bloch,', 'Bloch,', 'Bloch',\n","                            'Pearl spot', 'Pearls spot',\n","                            'Green chromide'],\n","            'Shol' : ['Sholo'],\n","            'Snapper' : ['Sinper', 'Sniper'],\n","            'White snapar' : ['White snapper'],\n","        }\n","\n","        for key, misspellings in misspelled_folders.items():\n","            if species_name in misspellings:\n","                return key\n","        return species_name\n","\n","    def download_data(self, date, keys):\n","        # Create a directory for the date if it doesn't exist\n","        date_directory = os.path.join(self.download_path, date)\n","        os.makedirs(date_directory, exist_ok=True)\n","\n","        # Download each object in the group\n","        for key in tqdm(keys, desc=f'Downloading {date} data'):\n","            # Extract fish name and fresh type from the image name\n","            image_name = key.split('/')[-1]\n","            try:\n","                fish_name, fresh_type = image_name.split('_')[-2:]\n","            except:\n","                continue\n","            fresh_type = fresh_type.split('.')[0]\n","            fish_name = fish_name.capitalize()\n","            fresh_type = fresh_type.capitalize()\n","\n","            if fish_name.endswith(\" \"):\n","                fish_name = fish_name[:-1]\n","            if fresh_type.endswith(\" \"):\n","                fresh_type = fresh_type[:-1]\n","\n","            fish_name = self.replace_misspelled_folder_names(fish_name)\n","\n","            # Create a directory structure:\n","            # date_folder/fish_name_folder/fresh_type_folder\n","            fish_directory = os.path.join(date_directory, fish_name)\n","            fresh_directory = os.path.join(fish_directory, fresh_type)\n","            os.makedirs(fresh_directory, exist_ok=True)\n","\n","            file_name = os.path.join(fresh_directory, image_name)\n","\n","            # Download the object if it doesn't already exist locally\n","            if not os.path.exists(file_name):\n","                self.s3_client.download_file(self.bucket_name, key, file_name)\n","            else:\n","                pass\n","#                 print(f\"Skipped (already exists): {file_name}\")\n","\n","    def download_daily_data(self):\n","        for date, keys in self.grouped_objects.items():\n","            self.download_data(date, keys)\n","\n","    def download_specific_date_data(self, specific_date):\n","        valid_dates = [date for date in self.grouped_objects.keys()]\n","        if specific_date not in valid_dates:\n","            print(f\"Data is not collected on : {specific_date}\")\n","            return\n","        keys = self.grouped_objects[specific_date]\n","        self.download_data(specific_date, keys)\n","\n","    def download_weekly_data(self, start_date, end_date):\n","        for date, keys in self.grouped_objects.items():\n","            if start_date <= date <= end_date:\n","                self.download_data(date, keys)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":391410,"status":"ok","timestamp":1696263478399,"user":{"displayName":"Test Qzense","userId":"10561881519641656595"},"user_tz":-330},"id":"qW8aIrub9j_n","outputId":"be803145-0575-4d0d-f5f4-bb3ee9e6c6c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","list_s3_objects done\n","group_objects_by_date done\n","\n"]},{"name":"stderr","output_type":"stream","text":["Downloading 2023-09-24 data: 100%|██████████| 197/197 [00:02<00:00, 84.01it/s] \n","Downloading 2023-09-26 data: 100%|██████████| 34/34 [00:19<00:00,  1.73it/s]\n","Downloading 2023-09-28 data: 100%|██████████| 7/7 [00:06<00:00,  1.07it/s]\n","Downloading 2023-09-29 data: 100%|██████████| 211/211 [02:13<00:00,  1.58it/s]\n","Downloading 2023-09-30 data: 100%|██████████| 222/222 [02:20<00:00,  1.58it/s]\n","Downloading 2023-10-02 data: 100%|██████████| 114/114 [01:12<00:00,  1.58it/s]\n"]}],"source":["# Example usage\n","bucket_name='fish-data-collection'\n","download_path=\"/content/drive/MyDrive/Sowmya /qZense Dataset/S3 Data/Daily Data\"\n","\n","data_downloader=S3DataDownloader(bucket_name, download_path)\n","\n","# Download all data daily\n","# data_downloader.download_daily_data()\n","\n","# Date should be in YYYY-MM-DD format\n","\n","# Download data for a specific date\n","# specific_date = '2023-09-09'\n","# data_downloader.download_specific_date_data(specific_date)\n","\n","# Download data weekly between two dates\n","start_date = '2023-09-24'\n","end_date = '2023-10-02'\n","data_downloader.download_weekly_data(start_date, end_date)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HL8VjM2dimdb"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMof8FYdYE0LK5qsI/AyyRB","cell_execution_strategy":"setup","mount_file_id":"1vV_DaepssNs2AVyB7yHoHaPkGSn1mlbQ","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
